version: 0.2
env:
  variables:
    REPOSITORY_URI: $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME
phases:
  install:
    runtime-versions: 
      docker: 20
    commands:
      - echo Installing kubectl...
      - curl -LO "https://dl.k8s.io/release/$(curl -sL https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
      - install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
      - echo installation successful
      - sudo chmod +x ./kubectl
      - mv ./kubectl /usr/local/bin/kubectl
      - whereis kubectl
      - which kubectl
      - echo Successfully installed kubectl
      - kubectl version --client
      - echo Installing authenticator...
      - curl -o aws-iam-authenticator https://amazon-eks.s3.us-east-1.amazonaws.com/1.15.10/2020-02-22/bin/linux/amd64/aws-iam-authenticator
      - chmod +x ./aws-iam-authenticator
      - sudo mv ./aws-iam-authenticator /usr/local/bin/aws-iam-authenticator
  pre_build:
    commands:
      - echo Setting up Kubernetes context...
      - aws eks update-kubeconfig --name my-eks-cluster --region $AWS_DEFAULT_REGION
      - CALLER_ARN=$(aws sts get-caller-identity --query 'Arn' --output text)
      - echo "Checking if EKS access entry exists for $CALLER_ARN..."
      - if ! aws eks describe-access-entry --cluster-name my-eks-cluster --principal-arn arn:aws:iam::$AWS_ACCOUNT_ID:role/eks-codebuild-role --output text | grep -q arn:aws:iam::$AWS_ACCOUNT_ID:role/eks-codebuild-role; then
          echo "Creating EKS access entry...";
          aws eks create-access-entry --cluster-name my-eks-cluster --principal-arn arn:aws:iam::$AWS_ACCOUNT_ID:role/eks-codebuild-role --type STANDARD --username arn:aws:sts::$AWS_ACCOUNT_ID:assumed-role/eks-codebuild-role/{{SessionName}};
          aws eks associate-access-policy --cluster-name my-eks-cluster --principal-arn arn:aws:iam::$AWS_ACCOUNT_ID:role/eks-codebuild-role --access-scope type=cluster --policy-arn arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy;
        else
          echo "EKS access entry for $CALLER_ARN already exists.";
        fi
      - |
        export SECURITY_GROUP_ID=$(aws ec2 describe-security-groups --filters "Name=group-name,Values=my-eks-cluster-node*" --query "SecurityGroups[*].GroupId" --output text)
            if [ -n "$SECURITY_GROUP_ID" ]; then
                TAG_EXISTS=$(aws ec2 describe-tags --filters "Name=resource-id,Values=$SECURITY_GROUP_ID" "Name=key,Values=kubernetes.io/cluster/my-eks-cluster" "Name=value,Values=owned" --query "Tags[*].Key" --output text)
                  if [ -n "$TAG_EXISTS" ]; then
                      aws ec2 delete-tags --resources "$SECURITY_GROUP_ID" --tags Key=kubernetes.io/cluster/my-eks-cluster,Value=owned
                      echo "Tag kubernetes.io/cluster/my-eks-cluster=owned deleted from security group $SECURITY_GROUP_ID"
                  else
                    echo "Tag kubernetes.io/cluster/my-eks-cluster=owned not found on security group $SECURITY_GROUP_ID"
                  fi
                
            else
                echo "No security group found with the name 'my-eks-cluster-node-*'"
            fi

  build:
    commands:
      - echo Passing the build phase
  post_build:
    commands:
      - echo "Deleting Kubernetes service and ingress to cleanup Load Balancer..."
      - kubectl delete svc my-app-service || true
      - kubectl delete ingress my-app-ingress || true
      - echo "Waiting for Load Balancer and associated Security Groups to be cleaned up..."
      - sleep 60  # Wait for AWS resources to clean up (adjust timing as needed)
      - kubectl get svc
artifacts:
  files:
    - '**/*'
  discard-paths: yes
